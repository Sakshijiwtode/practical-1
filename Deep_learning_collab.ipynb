{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNACF5LsWZl21zdAi1K5UFj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sakshijiwtode/practical-1/blob/main/Deep_learning_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning : Deep learning is a subset of machine learning, which is therefore a subset of artificial intelligence (AI). It is used to train itself to perform tasks, like speech and image recognition, by exposing multilayered neural networks to vast amounts of data."
      ],
      "metadata": {
        "id": "-i8RRHi0kkEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bias:A machine learning model analyzes data, looks for patterns and makes predictions. During training, the model learns these patterns in the dataset and applies them to the test data to make predictions. While making predictions, there is a difference between the predicted values of the model and the actual values and this difference is called bias. The bias is due to the assumptions of the model, which makes the objective function easy to learn. There are two types of bias: Low bias and high bias"
      ],
      "metadata": {
        "id": "641aqfgEmPvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple MLP model with biases\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # Linear layer with bias\n",
        "        self.relu = nn.ReLU()  # ReLU activation\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  # Another linear layer with bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)  # Apply first linear layer\n",
        "        out = self.relu(out)  # Apply ReLU activation\n",
        "        out = self.fc2(out)  # Apply second linear layer\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "input_size = 784  # Example input size (e.g., 28x28 images flattened)\n",
        "hidden_size = 500  # Number of neurons in the hidden layer\n",
        "num_classes = 10  # Number of output classes\n",
        "\n",
        "model = MLP(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Access the weights and biases of the first linear layer\n",
        "fc1_weights = model.fc1.weight\n",
        "fc1_biases = model.fc1.bias\n",
        "\n",
        "print(f\"First layer weights: {fc1_weights}\")\n",
        "print(f\"First layer biases: {fc1_biases}\")\n",
        "\n",
        "# Access the weights and biases of the second linear layer\n",
        "fc2_weights = model.fc2.weight\n",
        "fc2_biases = model.fc2.bias\n",
        "\n",
        "print(f\"Second layer weights: {fc2_weights}\")\n",
        "print(f\"Second layer biases: {fc2_biases}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZadkEFhmY9N",
        "outputId": "01d08c37-f831-472a-a531-dc8f6dfba2d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First layer weights: Parameter containing:\n",
            "tensor([[ 0.0146, -0.0045,  0.0286,  ...,  0.0208,  0.0057,  0.0095],\n",
            "        [-0.0007,  0.0245, -0.0049,  ..., -0.0060,  0.0126, -0.0333],\n",
            "        [ 0.0334,  0.0163,  0.0086,  ..., -0.0095,  0.0058, -0.0048],\n",
            "        ...,\n",
            "        [-0.0309, -0.0293,  0.0270,  ..., -0.0110, -0.0170, -0.0176],\n",
            "        [ 0.0236, -0.0283,  0.0097,  ...,  0.0200,  0.0102,  0.0001],\n",
            "        [ 0.0072,  0.0083,  0.0065,  ...,  0.0214,  0.0022, -0.0225]],\n",
            "       requires_grad=True)\n",
            "First layer biases: Parameter containing:\n",
            "tensor([-0.0152,  0.0061,  0.0195,  0.0007,  0.0298,  0.0196, -0.0239, -0.0039,\n",
            "         0.0201, -0.0028, -0.0115,  0.0179,  0.0043,  0.0156, -0.0337, -0.0344,\n",
            "        -0.0274,  0.0209,  0.0002, -0.0178, -0.0155,  0.0179,  0.0304, -0.0159,\n",
            "         0.0176,  0.0169, -0.0249,  0.0151, -0.0271, -0.0139,  0.0194, -0.0104,\n",
            "         0.0249, -0.0079,  0.0150,  0.0277,  0.0300, -0.0157,  0.0291,  0.0122,\n",
            "        -0.0009, -0.0021, -0.0255,  0.0227, -0.0194, -0.0231, -0.0306, -0.0169,\n",
            "        -0.0074,  0.0098,  0.0326,  0.0277,  0.0301,  0.0044, -0.0096,  0.0135,\n",
            "         0.0029,  0.0152, -0.0192, -0.0334,  0.0012,  0.0177,  0.0097, -0.0102,\n",
            "        -0.0078, -0.0318, -0.0187,  0.0221,  0.0106,  0.0304,  0.0119,  0.0349,\n",
            "         0.0338,  0.0069, -0.0323, -0.0024, -0.0034,  0.0313, -0.0195, -0.0165,\n",
            "         0.0153, -0.0158,  0.0139,  0.0017, -0.0151, -0.0122, -0.0246, -0.0337,\n",
            "         0.0142,  0.0307,  0.0113, -0.0146, -0.0050,  0.0021,  0.0203,  0.0005,\n",
            "         0.0265, -0.0215, -0.0083, -0.0125,  0.0267,  0.0275,  0.0212, -0.0245,\n",
            "         0.0337, -0.0126,  0.0156, -0.0229,  0.0171, -0.0325, -0.0120,  0.0262,\n",
            "        -0.0306,  0.0004,  0.0246, -0.0118,  0.0318,  0.0252, -0.0276,  0.0113,\n",
            "        -0.0143,  0.0356,  0.0169, -0.0025,  0.0311, -0.0168,  0.0060,  0.0024,\n",
            "        -0.0065,  0.0323, -0.0168, -0.0318, -0.0353,  0.0011, -0.0220,  0.0088,\n",
            "        -0.0133,  0.0315, -0.0066,  0.0019,  0.0014, -0.0333,  0.0191,  0.0122,\n",
            "         0.0291, -0.0127, -0.0245, -0.0002,  0.0046,  0.0173, -0.0318,  0.0274,\n",
            "        -0.0078,  0.0020,  0.0328, -0.0338,  0.0043,  0.0154,  0.0246,  0.0022,\n",
            "         0.0232,  0.0226, -0.0063, -0.0245,  0.0331, -0.0331, -0.0084, -0.0314,\n",
            "        -0.0071, -0.0291, -0.0129, -0.0340,  0.0234,  0.0131, -0.0310, -0.0054,\n",
            "         0.0261, -0.0215,  0.0185,  0.0025,  0.0279, -0.0295, -0.0190,  0.0124,\n",
            "        -0.0015, -0.0237,  0.0128,  0.0221, -0.0258, -0.0097,  0.0257,  0.0132,\n",
            "         0.0117, -0.0082, -0.0069, -0.0244,  0.0279, -0.0073, -0.0116, -0.0194,\n",
            "        -0.0333,  0.0237,  0.0059,  0.0215, -0.0353, -0.0341, -0.0087,  0.0140,\n",
            "         0.0082,  0.0123,  0.0241,  0.0272, -0.0227, -0.0131,  0.0086,  0.0058,\n",
            "         0.0120, -0.0054, -0.0353, -0.0236,  0.0032, -0.0029, -0.0223, -0.0179,\n",
            "        -0.0081,  0.0122,  0.0017,  0.0051, -0.0240, -0.0019, -0.0261,  0.0321,\n",
            "        -0.0320, -0.0010,  0.0293, -0.0083, -0.0159,  0.0039,  0.0117, -0.0103,\n",
            "         0.0151,  0.0014, -0.0076, -0.0351,  0.0208, -0.0056, -0.0341,  0.0068,\n",
            "        -0.0219,  0.0269,  0.0094,  0.0024,  0.0022, -0.0160, -0.0248,  0.0209,\n",
            "        -0.0085, -0.0075, -0.0172, -0.0089, -0.0269, -0.0236,  0.0008, -0.0284,\n",
            "         0.0237, -0.0010,  0.0224,  0.0251,  0.0072,  0.0288, -0.0229,  0.0106,\n",
            "        -0.0127,  0.0211,  0.0001,  0.0185, -0.0047,  0.0289, -0.0212, -0.0211,\n",
            "        -0.0247, -0.0087,  0.0022, -0.0035,  0.0094,  0.0038,  0.0089, -0.0165,\n",
            "        -0.0048, -0.0008,  0.0227,  0.0152, -0.0125, -0.0083,  0.0254, -0.0147,\n",
            "        -0.0280,  0.0109,  0.0005,  0.0262,  0.0071,  0.0297,  0.0222,  0.0255,\n",
            "         0.0043,  0.0108, -0.0344, -0.0204, -0.0074, -0.0290, -0.0140, -0.0261,\n",
            "         0.0345, -0.0105,  0.0085,  0.0294,  0.0023,  0.0229,  0.0021, -0.0177,\n",
            "         0.0278, -0.0340, -0.0311, -0.0193,  0.0096,  0.0255, -0.0239, -0.0318,\n",
            "        -0.0149, -0.0089, -0.0006,  0.0251, -0.0044,  0.0249,  0.0110,  0.0299,\n",
            "        -0.0192,  0.0139, -0.0109,  0.0255, -0.0311, -0.0091,  0.0124, -0.0024,\n",
            "        -0.0022, -0.0157,  0.0342, -0.0157, -0.0042, -0.0124,  0.0043,  0.0098,\n",
            "         0.0139, -0.0168, -0.0351,  0.0106,  0.0060,  0.0233,  0.0016, -0.0134,\n",
            "         0.0317, -0.0167, -0.0254, -0.0336,  0.0356, -0.0135,  0.0029, -0.0065,\n",
            "        -0.0321,  0.0250,  0.0355,  0.0129,  0.0090, -0.0075,  0.0014, -0.0202,\n",
            "         0.0073,  0.0071,  0.0213, -0.0143, -0.0128, -0.0212,  0.0020,  0.0017,\n",
            "        -0.0161, -0.0106,  0.0029, -0.0050,  0.0353,  0.0105, -0.0200,  0.0278,\n",
            "         0.0144, -0.0248,  0.0322,  0.0202, -0.0146, -0.0252, -0.0203, -0.0123,\n",
            "         0.0184,  0.0096,  0.0154,  0.0239, -0.0257, -0.0123, -0.0228, -0.0301,\n",
            "        -0.0175, -0.0305,  0.0067, -0.0091, -0.0323,  0.0183, -0.0120, -0.0052,\n",
            "         0.0308,  0.0346,  0.0291, -0.0258,  0.0101, -0.0102,  0.0218, -0.0043,\n",
            "         0.0231,  0.0052,  0.0322, -0.0084,  0.0288,  0.0124, -0.0351, -0.0313,\n",
            "         0.0012,  0.0106, -0.0326, -0.0213, -0.0214, -0.0188,  0.0024,  0.0048,\n",
            "        -0.0241,  0.0330, -0.0095,  0.0165, -0.0291, -0.0305,  0.0331,  0.0187,\n",
            "         0.0181,  0.0237, -0.0348,  0.0230, -0.0010, -0.0072,  0.0311, -0.0083,\n",
            "        -0.0178, -0.0151,  0.0256, -0.0006,  0.0175,  0.0067, -0.0324,  0.0075,\n",
            "         0.0278, -0.0197, -0.0212, -0.0226, -0.0049, -0.0121, -0.0127,  0.0295,\n",
            "         0.0047, -0.0278,  0.0058,  0.0085,  0.0032, -0.0018, -0.0046, -0.0276,\n",
            "        -0.0090,  0.0254,  0.0332,  0.0280,  0.0008,  0.0111, -0.0203, -0.0261,\n",
            "        -0.0103,  0.0135,  0.0097,  0.0200,  0.0297, -0.0291,  0.0215,  0.0323,\n",
            "        -0.0311, -0.0075, -0.0008,  0.0328], requires_grad=True)\n",
            "Second layer weights: Parameter containing:\n",
            "tensor([[ 0.0079, -0.0301,  0.0359,  ...,  0.0183, -0.0256,  0.0033],\n",
            "        [ 0.0256,  0.0241, -0.0181,  ..., -0.0269, -0.0265, -0.0394],\n",
            "        [ 0.0055, -0.0207,  0.0112,  ...,  0.0264,  0.0340,  0.0096],\n",
            "        ...,\n",
            "        [-0.0441,  0.0384, -0.0182,  ...,  0.0444, -0.0160,  0.0383],\n",
            "        [-0.0222,  0.0379,  0.0205,  ..., -0.0267,  0.0411, -0.0115],\n",
            "        [-0.0269,  0.0216,  0.0418,  ...,  0.0421,  0.0357,  0.0402]],\n",
            "       requires_grad=True)\n",
            "Second layer biases: Parameter containing:\n",
            "tensor([-0.0357, -0.0367, -0.0393, -0.0195,  0.0386,  0.0260,  0.0102, -0.0010,\n",
            "        -0.0400, -0.0323], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variance is a variance of data points that are spread. It quantifies how much the values in a data set differ from the average of the data set variance is simply refered to the variation in model predictions.\n",
        "There are two types High variance and Low variance\n",
        "High variance gives the scattered graph as there is more gap in prediction value ahe actual value\n",
        "Low variance give saturated graph as there is less gap in between predicted value and actual value."
      ],
      "metadata": {
        "id": "HAeROXSqnv3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample variance example"
      ],
      "metadata": {
        "id": "hI1utFJMoOA-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a sample tensor\n",
        "tensor = torch.tensor([[1.0, 2.0, 3.0],\n",
        "                       [4.0, 5.0, 6.0],\n",
        "                       [7.0, 8.0, 9.0]])\n",
        "\n",
        "# Compute the variance of the entire tensor\n",
        "var_all = torch.var(tensor)\n",
        "\n",
        "# Compute the variance along the rows (dim=0)\n",
        "var_rows = torch.var(tensor, dim=0)\n",
        "\n",
        "# Compute the variance along the columns (dim=1)\n",
        "var_cols = torch.var(tensor, dim=1)\n",
        "\n",
        "print(f\"Variance of the entire tensor: {var_all}\")\n",
        "print(f\"Variance along the rows: {var_rows}\")\n",
        "print(f\"Variance along the columns: {var_cols}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOjmwyB-nwZo",
        "outputId": "74898c08-55e4-4c61-eac7-8365a7446d1f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variance of the entire tensor: 7.5\n",
            "Variance along the rows: tensor([9., 9., 9.])\n",
            "Variance along the columns: tensor([1., 1., 1.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " MLP stands for Multilayer Perceptron it is a type of artificial neural network commonly used for supervised learning for ex classification and regression. It consists of 3 layers -Input layer,Hidden layer and output layer.\n",
        "Input.layer:It contains the neurons that receive the input features of the data. The number of neurons inthis layer is equal to the number of features in the dataset.\n",
        "Hidden layer:It is made up of neurons that perform intermediate computations and feature extraction. It can Have one or more hidden layers, each with a configurable number of neurons. The number of hidden layers and neurons is a hyperparameter that can be tuned.\n",
        "Output layer: It is responsible for final prediction and classification.The number of neurons in the output layer depends on the nature of the task\n"
      ],
      "metadata": {
        "id": "pHgmyOOTjES-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        # Define layers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input layer to hidden layer\n",
        "        self.relu = nn.ReLU()                         # ReLU activation function\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes) # Hidden layer to output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)  # Forward pass through the first layer\n",
        "        out = self.relu(out)  # Apply ReLU activation\n",
        "        out = self.fc2(out)  # Forward pass through the second layer\n",
        "        return out\n",
        "\n",
        "# Define the parameters\n",
        "input_size = 784  # Example input size (e.g., 28x28 images flattened)\n",
        "hidden_size = 500 # Number of neurons in the hidden layer\n",
        "num_classes = 10  # Number of output classes (e.g., for digit classification 0-9)\n",
        "num_epochs = 5    # Number of epochs to train\n",
        "batch_size = 100  # Batch size for training\n",
        "learning_rate = 0.001 # Learning rate\n",
        "\n",
        "# Load dataset (example using MNIST)\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = MLP(input_size, hidden_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Flatten the images\n",
        "        images = images.reshape(-1, 28*28)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxFgBOs7mFIl",
        "outputId": "86697833-28c2-4820-ede0-36e614a63d6f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.3951\n",
            "Epoch [1/5], Step [200/600], Loss: 0.3200\n",
            "Epoch [1/5], Step [300/600], Loss: 0.2200\n",
            "Epoch [1/5], Step [400/600], Loss: 0.2124\n",
            "Epoch [1/5], Step [500/600], Loss: 0.2034\n",
            "Epoch [1/5], Step [600/600], Loss: 0.0948\n",
            "Epoch [2/5], Step [100/600], Loss: 0.2644\n",
            "Epoch [2/5], Step [200/600], Loss: 0.1622\n",
            "Epoch [2/5], Step [300/600], Loss: 0.1933\n",
            "Epoch [2/5], Step [400/600], Loss: 0.2025\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0437\n",
            "Epoch [2/5], Step [600/600], Loss: 0.1799\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0683\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0941\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0534\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0473\n",
            "Epoch [3/5], Step [500/600], Loss: 0.1865\n",
            "Epoch [3/5], Step [600/600], Loss: 0.1185\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0438\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0692\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0739\n",
            "Epoch [4/5], Step [400/600], Loss: 0.1828\n",
            "Epoch [4/5], Step [500/600], Loss: 0.1764\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0533\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0813\n",
            "Epoch [5/5], Step [200/600], Loss: 0.1334\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0624\n",
            "Epoch [5/5], Step [400/600], Loss: 0.1526\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0147\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0914\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep neural network : A deep neural network (DNN) is an artificial neural network with many hidden layers between the input and output layers. These additional layers allow the network to model complex patterns in data, making DNNs powerful for varietyof tasks such as image and speech recognition, natural language processing &  more."
      ],
      "metadata": {
        "id": "oAuGawfOpqap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define the DNN model\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, num_classes):\n",
        "        super(DNN, self).__init__()\n",
        "        self.hidden_layers = nn.ModuleList()\n",
        "\n",
        "        # Input layer to first hidden layer\n",
        "        self.hidden_layers.append(nn.Linear(input_size, hidden_sizes[0]))\n",
        "        self.hidden_layers.append(nn.ReLU())\n",
        "\n",
        "        # Hidden layers\n",
        "        for i in range(len(hidden_sizes) - 1):\n",
        "            self.hidden_layers.append(nn.Linear(hidden_sizes[i], hidden_sizes[i + 1]))\n",
        "            self.hidden_layers.append(nn.ReLU())\n",
        "\n",
        "        # Output layer\n",
        "        self.output_layer = nn.Linear(hidden_sizes[-1], num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.hidden_layers:\n",
        "            x = layer(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "# Parameters\n",
        "input_size = 784  # 28x28 images flattened\n",
        "hidden_sizes = [512, 256, 128]  # Three hidden layers with respective sizes\n",
        "num_classes = 10  # Number of output classes (digits 0-9)\n",
        "num_epochs = 5  # Number of epochs\n",
        "batch_size = 100  # Batch size\n",
        "learning_rate = 0.001  # Learning rate\n",
        "\n",
        "# MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = DNN(input_size, hidden_sizes, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Flatten the images\n",
        "        images = images.view(-1, 28*28)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycJstGAIpySP",
        "outputId": "d3f283ee-b80a-4d3c-fd0d-496c3de669f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.5991\n",
            "Epoch [1/5], Step [200/600], Loss: 0.2362\n",
            "Epoch [1/5], Step [300/600], Loss: 0.2668\n",
            "Epoch [1/5], Step [400/600], Loss: 0.2280\n",
            "Epoch [1/5], Step [500/600], Loss: 0.2699\n",
            "Epoch [1/5], Step [600/600], Loss: 0.1616\n",
            "Epoch [2/5], Step [100/600], Loss: 0.1320\n",
            "Epoch [2/5], Step [200/600], Loss: 0.2438\n",
            "Epoch [2/5], Step [300/600], Loss: 0.1956\n",
            "Epoch [2/5], Step [400/600], Loss: 0.1907\n",
            "Epoch [2/5], Step [500/600], Loss: 0.1847\n",
            "Epoch [2/5], Step [600/600], Loss: 0.2926\n",
            "Epoch [3/5], Step [100/600], Loss: 0.2329\n",
            "Epoch [3/5], Step [200/600], Loss: 0.1633\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0579\n",
            "Epoch [3/5], Step [400/600], Loss: 0.1022\n",
            "Epoch [3/5], Step [500/600], Loss: 0.1030\n",
            "Epoch [3/5], Step [600/600], Loss: 0.1596\n",
            "Epoch [4/5], Step [100/600], Loss: 0.0780\n",
            "Epoch [4/5], Step [200/600], Loss: 0.1263\n",
            "Epoch [4/5], Step [300/600], Loss: 0.1239\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0967\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0445\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0980\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0449\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0692\n",
            "Epoch [5/5], Step [300/600], Loss: 0.1492\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0663\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0691\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0333\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backpropagation is a fundamental algorithm used in training artificial neural networks, particularly in the context of supervised learning. It efficiently computes the gradient of the loss function with respect to each weight in the network, allowing for the optimization of the model through gradient descent."
      ],
      "metadata": {
        "id": "2aOFnbyiq1Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a toy dataset\n",
        "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define a simple neural network\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Model parameters\n",
        "input_size = 2\n",
        "hidden_size = 5\n",
        "num_classes = 2\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = SimpleNN(input_size, hidden_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Clear gradients\n",
        "    loss.backward()  # Compute gradients\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SC_YQl3rU8L",
        "outputId": "5f6e7729-0c5d-4d53-94ba-b82fffb9cd5a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.6864\n",
            "Epoch [20/100], Loss: 0.6811\n",
            "Epoch [30/100], Loss: 0.6758\n",
            "Epoch [40/100], Loss: 0.6706\n",
            "Epoch [50/100], Loss: 0.6654\n",
            "Epoch [60/100], Loss: 0.6603\n",
            "Epoch [70/100], Loss: 0.6551\n",
            "Epoch [80/100], Loss: 0.6498\n",
            "Epoch [90/100], Loss: 0.6445\n",
            "Epoch [100/100], Loss: 0.6392\n",
            "Training complete.\n",
            "Accuracy: 76.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ActivationThe activation function must decide whether a neuron should be activated or not by calculating a weighted sum of its inputs and adding a bias. This process allows the network to learn and perform more complex tasks by changing the input in a non-linear fashion. Essentially, activation functions help mode complex relationships in data, which is crucial for tasks like classification and regression.\n"
      ],
      "metadata": {
        "id": "KvnZ-dO_rxiL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid\n",
        "Range: 0 to 1\n",
        "Use: Often used in the output layer for binary classification tasks. However, it can lead to issues like vanishing gradients during training."
      ],
      "metadata": {
        "id": "92uPcLQvsp_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Input tensor\n",
        "x = torch.tensor([[-1.0, 2.0, -3.0], [4.0, -5.0, 6.0]])\n",
        "\n",
        "# Sigmoid activation function\n",
        "sigmoid = nn.Sigmoid()\n",
        "output = sigmoid(x)\n",
        "\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"Output after Sigmoid:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSQAITpAryMB",
        "outputId": "27cb3b02-05ba-461d-a7f2-d025f2ac912f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[-1.,  2., -3.],\n",
            "        [ 4., -5.,  6.]])\n",
            "Output after Sigmoid:\n",
            " tensor([[0.2689, 0.8808, 0.0474],\n",
            "        [0.9820, 0.0067, 0.9975]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU (Rectified Linear Unit)\n",
        "Range: 0 to ∞\n",
        "Use: Widely used in hidden layers of deep networks due to its simplicity and effectiveness in mitigating the vanishing gradient problem"
      ],
      "metadata": {
        "id": "t5-cItjJtCGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Input tensor\n",
        "x = torch.tensor([[-1.0, 2.0, -3.0], [4.0, -5.0, 6.0]])\n",
        "\n",
        "# ReLU activation function\n",
        "relu = nn.ReLU()\n",
        "output = relu(x)\n",
        "\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"Output after ReLU:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhQgzmC6tDHN",
        "outputId": "50de3c84-1b30-48f8-f6a3-516d6ac31b80"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[-1.,  2., -3.],\n",
            "        [ 4., -5.,  6.]])\n",
            "Output after ReLU:\n",
            " tensor([[0., 2., 0.],\n",
            "        [4., 0., 6.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tanh (Hyperbolic Tangent) Function:\n",
        "Range: -1 to 1\n",
        "Use: Generally preferred over the sigmoid function due to its zero-centered output, which helps in faster convergence during training."
      ],
      "metadata": {
        "id": "iB5fJ8ymtisl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Input tensor\n",
        "x = torch.tensor([[-1.0, 2.0, -3.0], [4.0, -5.0, 6.0]])\n",
        "\n",
        "# Tanh activation function\n",
        "tanh = nn.Tanh()\n",
        "output = tanh(x)\n",
        "\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"Output after Tanh:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLhCKfeFtjJl",
        "outputId": "d6b14618-ed9c-4be3-b345-03e2d52e20a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[-1.,  2., -3.],\n",
            "        [ 4., -5.,  6.]])\n",
            "Output after Tanh:\n",
            " tensor([[-0.7616,  0.9640, -0.9951],\n",
            "        [ 0.9993, -0.9999,  1.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leaky ReLU:\n",
        "Range: −∞ to +∞\n",
        "Use: Addresses the \"dying ReLU\" problem by allowing a small gradient when the unit is not active."
      ],
      "metadata": {
        "id": "adpWtpjht6WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Input tensor\n",
        "x = torch.tensor([[-1.0, 2.0, -3.0], [4.0, -5.0, 6.0]])\n",
        "\n",
        "# Leaky ReLU activation function\n",
        "leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
        "output = leaky_relu(x)\n",
        "\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"Output after Leaky ReLU:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKAw_Njzt67h",
        "outputId": "290c0b5b-20dd-47b3-a2fc-4e4fcd57b5ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[-1.,  2., -3.],\n",
            "        [ 4., -5.,  6.]])\n",
            "Output after Leaky ReLU:\n",
            " tensor([[-0.0100,  2.0000, -0.0300],\n",
            "        [ 4.0000, -0.0500,  6.0000]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax Function:\n",
        "Range: 0 to 1 (sums to 1 across multiple outputs)\n",
        "Use: Commonly used in the output layer for multi-class classification tasks."
      ],
      "metadata": {
        "id": "ODA02pyzuSSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Input tensor\n",
        "x = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "\n",
        "# Softmax activation function\n",
        "softmax = nn.Softmax(dim=1)\n",
        "output = softmax(x)\n",
        "\n",
        "print(\"Input:\\n\", x)\n",
        "print(\"Output after Softmax:\\n\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwdMDt9auS0C",
        "outputId": "4ceca367-5fec-46c3-c9f6-6445b38ea776"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " tensor([[1., 2., 3.],\n",
            "        [4., 5., 6.]])\n",
            "Output after Softmax:\n",
            " tensor([[0.0900, 0.2447, 0.6652],\n",
            "        [0.0900, 0.2447, 0.6652]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization: Regularization is a set of techniques that prevent machine learning models from overfitting by adding additional information or constraints to the model. Overfitting occurs when the model learns not only the underlying patterns in the training data but also noise and outliers, resulting in poor generalization from unseen data. Regularization helps improve the generalizability of the model by penalizing complex models. Here are some of the more common legalization techniques.\n",
        "L1 Regularization (Lasso),L2 Regularization (Ridge),Elastic Net,Dropout and Weight Decay."
      ],
      "metadata": {
        "id": "dQfBjPK2ubj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization techniques are used in machine learning to prevent overfitting by adding a penalty to the loss function. Two common regularization techniques are L2 regularization (also known as weight decay) and Dropout. Below is an example of how to implement these techniques in PyTorch.L2 regularization and Dropout to a simple neural network model."
      ],
      "metadata": {
        "id": "jXkzIv7-vfWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate a toy dataset\n",
        "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Define a simple neural network with Dropout\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)  # Dropout with 50% probability\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "# Model parameters\n",
        "input_size = 2\n",
        "hidden_size = 5\n",
        "num_classes = 2\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "weight_decay = 0.001  # L2 regularization parameter\n",
        "\n",
        "# Initialize the model, loss function, and optimizer with weight decay\n",
        "model = SimpleNN(input_size, hidden_size, num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # Backward pass and optimization\n",
        "    optimizer.zero_grad()  # Clear gradients\n",
        "    loss.backward()  # Compute gradients\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K54ZzirhveZy",
        "outputId": "ba96043d-8759-47e1-b237-58fb6000a172"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.8680\n",
            "Epoch [20/100], Loss: 0.6897\n",
            "Epoch [30/100], Loss: 0.5606\n",
            "Epoch [40/100], Loss: 0.4819\n",
            "Epoch [50/100], Loss: 0.4539\n",
            "Epoch [60/100], Loss: 0.4536\n",
            "Epoch [70/100], Loss: 0.4498\n",
            "Epoch [80/100], Loss: 0.4077\n",
            "Epoch [90/100], Loss: 0.4064\n",
            "Epoch [100/100], Loss: 0.4322\n",
            "Training complete.\n",
            "Accuracy: 89.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Networks (CNNs) are a specialized deep learning architecture used primarily for computer vision tasks such as image classification and object detection. They are designed to automatically and adaptively learn spatial hierarchies of features from input images."
      ],
      "metadata": {
        "id": "pAzrWxU5wFFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # Convolutional layer 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        # Convolutional layer 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        # Fully connected layer 1\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 100)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "        # Fully connected layer 2 (output layer)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply first convolutional layer, ReLU and max pooling\n",
        "        x = self.pool(self.relu1(self.conv1(x)))\n",
        "\n",
        "        # Apply second convolutional layer, ReLU and max pooling\n",
        "        x = self.pool(self.relu2(self.conv2(x)))\n",
        "\n",
        "        # Flatten the output from the conv layers\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "\n",
        "        # Apply first fully connected layer and ReLU\n",
        "        x = self.relu3(self.fc1(x))\n",
        "\n",
        "        # Apply second fully connected layer (output layer)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Transformations for the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "# Model, loss function, optimizer\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Accuracy of the model on the 10000 test images: {100 * correct / total:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIyhNhxqwzjU",
        "outputId": "6cc48c12-b203-4d66-c23b-ec19739b94a0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.0574\n",
            "Epoch [2/5], Loss: 0.0580\n",
            "Epoch [3/5], Loss: 0.0210\n",
            "Epoch [4/5], Loss: 0.0236\n",
            "Epoch [5/5], Loss: 0.0017\n",
            "Accuracy of the model on the 10000 test images: 99.07%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "RNNs: Recurrent Neural Networks (RNNs) are a class of artificial neural networks designed to process sequential data, making them particularly effective for tasks that involve time series, natural language, and other ordered data. Unlike traditional feedforward neural networks, RNNs have connections that loop back on themselves, allowing them to maintain a form of memory about previous inputs."
      ],
      "metadata": {
        "id": "Zlhkp0Sswipq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Prepare data\n",
        "text = \"hello pytorch, this is an example of rnn implementation.\"\n",
        "chars = sorted(list(set(text)))\n",
        "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
        "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
        "data_size, vocab_size = len(text), len(chars)\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = vocab_size\n",
        "hidden_size = 128\n",
        "output_size = vocab_size\n",
        "num_layers = 1\n",
        "learning_rate = 0.01\n",
        "num_epochs = 500\n",
        "\n",
        "# Prepare inputs and targets\n",
        "seq_length = 5\n",
        "X_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, data_size - seq_length):\n",
        "    X_seq = text[i:i + seq_length]\n",
        "    y_seq = text[i + 1:i + seq_length + 1]\n",
        "    X_data.append([char_to_ix[ch] for ch in X_seq])\n",
        "    y_data.append([char_to_ix[ch] for ch in y_seq])\n",
        "\n",
        "X_data = torch.tensor(X_data, dtype=torch.long)\n",
        "y_data = torch.tensor(y_data, dtype=torch.long)\n",
        "\n",
        "# Define the RNN model\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.embed = nn.Embedding(input_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
        "        x = self.embed(x)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = RNN(input_size, hidden_size, output_size, num_layers)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    outputs = model(X_data)\n",
        "    loss = criterion(outputs.view(-1, vocab_size), y_data.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 50 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    # Start with the first character\n",
        "    input_seq = text[:seq_length]\n",
        "    input_seq_ix = [char_to_ix[ch] for ch in input_seq]\n",
        "    input_seq_tensor = torch.tensor([input_seq_ix], dtype=torch.long)\n",
        "\n",
        "    predicted_seq = input_seq\n",
        "\n",
        "    for _ in range(100):\n",
        "        output = model(input_seq_tensor)\n",
        "        output = output[:, -1, :]\n",
        "        _, predicted_ix = torch.max(output, 1)\n",
        "        predicted_char = ix_to_char[predicted_ix.item()]\n",
        "        predicted_seq += predicted_char\n",
        "\n",
        "        input_seq_tensor = torch.tensor([[char_to_ix[ch] for ch in predicted_seq[-seq_length:]]], dtype=torch.long)\n",
        "\n",
        "    print(f'Predicted sequence: {predicted_seq}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz-iqtQjxA-_",
        "outputId": "778b7acc-48a0-483f-9e38-a80b9ace7980"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [50/500], Loss: 0.2428\n",
            "Epoch [100/500], Loss: 0.2421\n",
            "Epoch [150/500], Loss: 0.2420\n",
            "Epoch [200/500], Loss: 0.2420\n",
            "Epoch [250/500], Loss: 0.2418\n",
            "Epoch [300/500], Loss: 0.2418\n",
            "Epoch [350/500], Loss: 0.2418\n",
            "Epoch [400/500], Loss: 0.2418\n",
            "Epoch [450/500], Loss: 0.2417\n",
            "Epoch [500/500], Loss: 0.2418\n",
            "Predicted sequence: hello pytorch, this is an example of rnn implementation.ementation.ementation.ementation.ementation.ement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer: Transformer Architecture is a model that uses self-attention that transforms one whole sentence into a single sentence. This is a big shift from how older models work step by step, and it helps overcome the challenges seen in models like RNNs and LSTMs"
      ],
      "metadata": {
        "id": "8W-W_v5gxBrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Sample data\n",
        "src_vocab_size = 10\n",
        "tgt_vocab_size = 10\n",
        "src_seq_length = 6\n",
        "tgt_seq_length = 6\n",
        "batch_size = 2\n",
        "\n",
        "src = torch.randint(0, src_vocab_size, (batch_size, src_seq_length))\n",
        "tgt = torch.randint(0, tgt_vocab_size, (batch_size, tgt_seq_length))\n",
        "\n",
        "# Define the Transformer model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_encoder = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model, d_model)\n",
        "        )\n",
        "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src = self.src_embedding(src) * torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
        "        tgt = self.tgt_embedding(tgt) * torch.sqrt(torch.tensor(d_model, dtype=torch.float32))\n",
        "        src = self.pos_encoder(src)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "        src = src.permute(1, 0, 2)\n",
        "        tgt = tgt.permute(1, 0, 2)\n",
        "        memory = self.transformer.encoder(src)\n",
        "        out = self.transformer.decoder(tgt, memory)\n",
        "        out = self.fc_out(out.permute(1, 0, 2))\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "d_model = 512\n",
        "nhead = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dim_feedforward = 2048\n",
        "dropout = 0.1\n",
        "learning_rate = 0.001\n",
        "num_epochs = 20\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    output = model(src, tgt[:, :-1])\n",
        "    loss = criterion(output.reshape(-1, tgt_vocab_size), tgt[:, 1:].reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Testing the model (just a forward pass for demonstration)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(src, tgt[:, :-1])\n",
        "    print(\"Output shape:\", output.shape)\n",
        "    print(\"Output:\", output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRlxtLcVxL2e",
        "outputId": "56890635-2b72-4e0b-9d19-8afca04ae73d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/20], Loss: 2.2506\n",
            "Epoch [10/20], Loss: 1.7717\n",
            "Epoch [15/20], Loss: 1.7431\n",
            "Epoch [20/20], Loss: 1.7012\n",
            "Output shape: torch.Size([2, 5, 10])\n",
            "Output: tensor([[[-4.2054, -3.7651,  2.8994,  2.6815, -4.9354,  1.9238,  1.3583,\n",
            "          -3.6667,  3.2336,  3.2356],\n",
            "         [-4.2066, -3.7646,  2.8985,  2.6841, -4.9356,  1.9220,  1.3588,\n",
            "          -3.6666,  3.2346,  3.2345],\n",
            "         [-4.2066, -3.7646,  2.8985,  2.6841, -4.9356,  1.9220,  1.3588,\n",
            "          -3.6666,  3.2346,  3.2345],\n",
            "         [-4.2045, -3.7647,  2.8989,  2.6802, -4.9339,  1.9208,  1.3555,\n",
            "          -3.6660,  3.2280,  3.2458],\n",
            "         [-4.2006, -3.7674,  2.9110,  2.6681, -4.9379,  1.9433,  1.3614,\n",
            "          -3.6669,  3.2213,  3.2283]],\n",
            "\n",
            "        [[-4.1661, -3.7812,  2.8991,  2.4247, -4.9598,  2.1199,  1.4273,\n",
            "          -3.6680,  3.2238,  3.1951],\n",
            "         [-4.1682, -3.7815,  2.8983,  2.4292, -4.9599,  2.1163,  1.4277,\n",
            "          -3.6687,  3.2237,  3.1949],\n",
            "         [-4.1639, -3.7821,  2.9047,  2.4210, -4.9598,  2.1256,  1.4266,\n",
            "          -3.6677,  3.2153,  3.1957],\n",
            "         [-4.1643, -3.7811,  2.9001,  2.4224, -4.9576,  2.1182,  1.4227,\n",
            "          -3.6667,  3.2139,  3.2088],\n",
            "         [-4.1637, -3.7820,  2.9043,  2.4207, -4.9601,  2.1278,  1.4264,\n",
            "          -3.6675,  3.2152,  3.1952]]])\n"
          ]
        }
      ]
    }
  ]
}